{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5698d802",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11fd967",
   "metadata": {},
   "source": [
    "### List all the categories available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cf90e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### read the json file\n",
    "import json\n",
    "cats = json.load(open('/net/acadia10a/data/sparsh/mapillary/mapillary-2.0/config_v2.0.json', 'r'))\n",
    "cats = {i['readable']: i['color'] for i in cats['labels']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3775a",
   "metadata": {},
   "source": [
    "### Fetch vehicles and visualize the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1eb0b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### read the polygon file\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "vehicles_categories = {\n",
    "  \"Bicycle\": [119, 11, 32],\n",
    "  \"Boat\": [150, 0, 255],\n",
    "  \"Bus\": [0, 60, 100],\n",
    "  \"Car\": [0, 0, 142],\n",
    "  \"Caravan\": [0, 0, 90],\n",
    "  \"Motorcycle\": [0, 0, 230],\n",
    "  \"On Rails\": [0, 80, 100],\n",
    "  \"Other Vehicle\": [128, 64, 64],\n",
    "  \"Trailer\": [0, 0, 110],\n",
    "  \"Truck\": [0, 0, 70],\n",
    "  \"Vehicle Group\": [0, 0, 142],\n",
    "  \"Wheeled Slow\": [0, 0, 192],\n",
    "  \"Car Mount\": [32, 32, 32]\n",
    "}\n",
    "def read_polygon(polygon_path):\n",
    "    polys = json.load(open(polygon_path, 'r'))\n",
    "    vehicle_instances = {}\n",
    "    for i in polys['objects']:\n",
    "        if 'vehicle' in i['label']:\n",
    "            if i['label'] not in vehicle_instances:\n",
    "                vehicle_instances[i['label']] = [i['polygon']]\n",
    "            else:\n",
    "                vehicle_instances[i['label']].append(i['polygon'])\n",
    "    return vehicle_instances\n",
    "\n",
    "def visualization(image_path, all_polygon_points):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Draw polygon\n",
    "    for key, polygon_points in all_polygon_points.items():\n",
    "        for poly in polygon_points:\n",
    "            poly = np.array(poly).astype(np.int32)\n",
    "            cv2.polylines(image, [poly], isClosed=True, color=(0, 255, 0), thickness=5)\n",
    "            x, y, w, h = cv2.boundingRect(poly)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cv2.putText(image, key, (x, y - 10), font, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    return Image.fromarray(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9dad3",
   "metadata": {},
   "source": [
    "### Load InternVL to relabel the vehicles into following categories: \n",
    "Car\n",
    "\n",
    "Bus\n",
    "\n",
    "Truck\n",
    "\n",
    "Bicycle\n",
    "\n",
    "Motorcycle\n",
    "\n",
    "Boat\n",
    "\n",
    "Trailer\n",
    "\n",
    "Caravan\n",
    "\n",
    "On Rails\n",
    "\n",
    "Other Vehicle\n",
    "\n",
    "Vehicle Group\n",
    "\n",
    "Wheeled Slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "567ceb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6cb732067b4fe4978c172d96522dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.43 GiB of which 5.69 MiB is free. Process 114061 has 16.41 GiB memory in use. Including non-PyTorch memory, this process has 30.99 GiB memory in use. Of the allocated memory 30.19 GiB is allocated by PyTorch, and 305.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 86\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pixel_values\n\u001b[1;32m     80\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenGVLab/InternVL2_5-8B-MPO\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     81\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_flash_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m tokenizer1 \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(path, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     89\u001b[0m prompt1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m<image>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou are an expert in autonomous driving, specializing in analyzing traffic scenes.                \u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124m            Your task is to look at the obstacle in the red bbox inside green polygon  and output the response in the format below. think step by step before deciding the answer\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124m            Strictly follow the rules.\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124m                \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Choose ONE category among the following for the most dominant object in the bounding box: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcar\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbus\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruck\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicycle\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmotorbicycle\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboat\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrailer\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\t\t                     \u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124m            }\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/clip3/lib/python3.12/site-packages/transformers/modeling_utils.py:3117\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3112\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3113\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `cuda()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3114\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3115\u001b[0m         )\n\u001b[1;32m   3116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/clip3/lib/python3.12/site-packages/torch/nn/modules/module.py:911\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/clip3/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/clip3/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/clip3/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/clip3/lib/python3.12/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/clip3/lib/python3.12/site-packages/torch/nn/modules/module.py:911\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.43 GiB of which 5.69 MiB is free. Process 114061 has 16.41 GiB memory in use. Including non-PyTorch memory, this process has 30.99 GiB memory in use. Of the allocated memory 30.19 GiB is allocated by PyTorch, and 305.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image, input_size=448, max_num=12):\n",
    "    #image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "path = 'OpenGVLab/InternVL2_5-8B-MPO'\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True).eval().cuda()\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "prompt1 = \"\"\"<image>\\nYou are an expert in autonomous driving, specializing in analyzing traffic scenes.                \n",
    "            Your task is to look at the obstacle in the red bbox inside green polygon  and output the response in the format below. think step by step before deciding the answer\n",
    "            Strictly follow the rules.\n",
    "            {\n",
    "                \"category\": \"[Choose ONE category among the following for the most dominant object in the bounding box: \"car\", \"bus\", \"truck\", \"bicycle\", \"motorbicycle\", \"boat\", \"trailer\", \"train\"]\"\t\t                     \n",
    "            }\"\"\"\n",
    "\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "all_polygon_points = read_polygon('/net/acadia10a/data/sparsh/mapillary/mapillary-2.0/validation/v2.0/polygons/mURdvO8w-pV8dqsqLntWCQ.json')\n",
    "image_path = '/net/acadia10a/data/sparsh/mapillary/mapillary-2.0/validation/images/mURdvO8w-pV8dqsqLntWCQ.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "for key, polygon_points in all_polygon_points.items():\n",
    "    for poly in polygon_points:\n",
    "        poly = np.array(poly).astype(np.int32)\n",
    "        cv2.polylines(image, [poly], isClosed=True, color=(0, 255, 0), thickness=5)\n",
    "        x, y, w, h = cv2.boundingRect(poly)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        cropped_image = image[y:y + h, x:x + w] \n",
    "        #display(Image.fromarray(cropped_image))\n",
    "        pixel_values = load_image(Image.fromarray(cropped_image), max_num=12).to(torch.bfloat16).cuda()\n",
    "        response11 = model.chat(tokenizer1, pixel_values, prompt1, generation_config)\n",
    "        print(\"Ground Truth:\", key)\n",
    "        print(\"Prediction:\", response11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a7ae4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74aa39ae0b584543971bf219f479df3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312b7fb07c444be984df256e79bed7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a25075b3cd4278a2ecc0f73fe211ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6907ff26e25e4c5cbabb76e93c3feaf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ee161e36bd4f6e97346f5871beb122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9caa39cfb841b1b60e2575c56c7e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0a70bcee9c46d0ad5f26e1d5cdd38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma/sparsh/anaconda3/envs/clip3/lib/python3.12/site-packages/accelerate/utils/modeling.py:1384: UserWarning: Current model requires 3712 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d72ea625d084e27817725ae062f8087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e4cefb4b31401c808190cd7a6def7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44bf0ce774a44c5b1de29f643ae4d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808f470644314098af3dcade9341e77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acf5ae986034225b8c0dc3bdac63588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c486117f0b460fba952a836b3c7b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "780daddf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Qwen2_5_VLForConditionalGeneration' from 'transformers' (/home/ma/sparsh/anaconda3/envs/clip3/lib/python3.12/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqwen_vl_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process_vision_info\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# default: Load the model on the available device(s)\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Qwen2_5_VLForConditionalGeneration' from 'transformers' (/home/ma/sparsh/anaconda3/envs/clip3/lib/python3.12/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a69cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open('/net/acadia12a/data/samuel/objects365v1/zsy_objv1_train_xdino.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e20e7435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'annotations', 'categories', 'licenses'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87373948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'person', 'id': 1},\n",
       " {'name': 'sneakers', 'id': 2},\n",
       " {'name': 'chair', 'id': 3},\n",
       " {'name': 'other shoes', 'id': 4},\n",
       " {'name': 'hat', 'id': 5},\n",
       " {'name': 'car', 'id': 6},\n",
       " {'name': 'lamp', 'id': 7},\n",
       " {'name': 'glasses', 'id': 8},\n",
       " {'name': 'bottle', 'id': 9},\n",
       " {'name': 'desk', 'id': 10},\n",
       " {'name': 'cup', 'id': 11},\n",
       " {'name': 'streetlight', 'id': 12},\n",
       " {'name': 'cabinet / shelf', 'id': 13},\n",
       " {'name': 'handbag', 'id': 14},\n",
       " {'name': 'bracelet', 'id': 15},\n",
       " {'name': 'plate', 'id': 16},\n",
       " {'name': 'picture / frame', 'id': 17},\n",
       " {'name': 'helmet', 'id': 18},\n",
       " {'name': 'book', 'id': 19},\n",
       " {'name': 'gloves', 'id': 20},\n",
       " {'name': 'storage box', 'id': 21},\n",
       " {'name': 'boat', 'id': 22},\n",
       " {'name': 'leather shoes', 'id': 23},\n",
       " {'name': 'flower', 'id': 24},\n",
       " {'name': 'bench', 'id': 25},\n",
       " {'name': 'potted plant', 'id': 26},\n",
       " {'name': 'bowl', 'id': 27},\n",
       " {'name': 'flag', 'id': 28},\n",
       " {'name': 'pillow', 'id': 29},\n",
       " {'name': 'boots', 'id': 30},\n",
       " {'name': 'vase', 'id': 31},\n",
       " {'name': 'microphone', 'id': 32},\n",
       " {'name': 'necklace', 'id': 33},\n",
       " {'name': 'ring', 'id': 34},\n",
       " {'name': 'suv', 'id': 35},\n",
       " {'name': 'wine glass', 'id': 36},\n",
       " {'name': 'belt', 'id': 37},\n",
       " {'name': 'monitor / tv', 'id': 38},\n",
       " {'name': 'backpack', 'id': 39},\n",
       " {'name': 'umbrella', 'id': 40},\n",
       " {'name': 'traffic light', 'id': 41},\n",
       " {'name': 'speaker', 'id': 42},\n",
       " {'name': 'watch', 'id': 43},\n",
       " {'name': 'tie', 'id': 44},\n",
       " {'name': 'trashcan', 'id': 45},\n",
       " {'name': 'slippers', 'id': 46},\n",
       " {'name': 'bicycle', 'id': 47},\n",
       " {'name': 'stool', 'id': 48},\n",
       " {'name': 'barrel / bucket', 'id': 49},\n",
       " {'name': 'van', 'id': 50},\n",
       " {'name': 'couch', 'id': 51},\n",
       " {'name': 'sandals', 'id': 52},\n",
       " {'name': 'basket', 'id': 53},\n",
       " {'name': 'drum', 'id': 54},\n",
       " {'name': 'pen / pencil', 'id': 55},\n",
       " {'name': 'bus', 'id': 56},\n",
       " {'name': 'wild bird', 'id': 57},\n",
       " {'name': 'high heels', 'id': 58},\n",
       " {'name': 'motorcycle', 'id': 59},\n",
       " {'name': 'guitar', 'id': 60},\n",
       " {'name': 'carpet', 'id': 61},\n",
       " {'name': 'cell phone', 'id': 62},\n",
       " {'name': 'bread', 'id': 63},\n",
       " {'name': 'camera', 'id': 64},\n",
       " {'name': 'canned', 'id': 65},\n",
       " {'name': 'truck', 'id': 66},\n",
       " {'name': 'traffic cone', 'id': 67},\n",
       " {'name': 'cymbal', 'id': 68},\n",
       " {'name': 'lifesaver', 'id': 69},\n",
       " {'name': 'towel', 'id': 70},\n",
       " {'name': 'teddybear', 'id': 71},\n",
       " {'name': 'candle', 'id': 72},\n",
       " {'name': 'sailboat', 'id': 73},\n",
       " {'name': 'laptop', 'id': 74},\n",
       " {'name': 'awning', 'id': 75},\n",
       " {'name': 'bed', 'id': 76},\n",
       " {'name': 'faucet', 'id': 77},\n",
       " {'name': 'tent', 'id': 78},\n",
       " {'name': 'horse', 'id': 79},\n",
       " {'name': 'mirror', 'id': 80},\n",
       " {'name': 'power outlet', 'id': 81},\n",
       " {'name': 'sink', 'id': 82},\n",
       " {'name': 'apple', 'id': 83},\n",
       " {'name': 'air conditioner', 'id': 84},\n",
       " {'name': 'knife', 'id': 85},\n",
       " {'name': 'hockey stick', 'id': 86},\n",
       " {'name': 'paddle', 'id': 87},\n",
       " {'name': 'pickup truck', 'id': 88},\n",
       " {'name': 'fork', 'id': 89},\n",
       " {'name': 'traffic sign', 'id': 90},\n",
       " {'name': 'ballon', 'id': 91},\n",
       " {'name': 'tripod', 'id': 92},\n",
       " {'name': 'dog', 'id': 93},\n",
       " {'name': 'spoon', 'id': 94},\n",
       " {'name': 'clock', 'id': 95},\n",
       " {'name': 'pot', 'id': 96},\n",
       " {'name': 'cow', 'id': 97},\n",
       " {'name': 'cake', 'id': 98},\n",
       " {'name': 'dining table', 'id': 99},\n",
       " {'name': 'sheep', 'id': 100},\n",
       " {'name': 'hanger', 'id': 101},\n",
       " {'name': 'blackboard / whiteboard', 'id': 102},\n",
       " {'name': 'napkin', 'id': 103},\n",
       " {'name': 'other fish', 'id': 104},\n",
       " {'name': 'orange', 'id': 105},\n",
       " {'name': 'toiletry', 'id': 106},\n",
       " {'name': 'keyboard', 'id': 107},\n",
       " {'name': 'tomato', 'id': 108},\n",
       " {'name': 'lantern', 'id': 109},\n",
       " {'name': 'machinery vehicle', 'id': 110},\n",
       " {'name': 'fan', 'id': 111},\n",
       " {'name': 'green vegetables', 'id': 112},\n",
       " {'name': 'banana', 'id': 113},\n",
       " {'name': 'baseball glove', 'id': 114},\n",
       " {'name': 'airplane', 'id': 115},\n",
       " {'name': 'mouse', 'id': 116},\n",
       " {'name': 'train', 'id': 117},\n",
       " {'name': 'pumpkin', 'id': 118},\n",
       " {'name': 'soccer', 'id': 119},\n",
       " {'name': 'skis', 'id': 120},\n",
       " {'name': 'suitcase', 'id': 121},\n",
       " {'name': 'nightstand', 'id': 122},\n",
       " {'name': 'teapot', 'id': 123},\n",
       " {'name': 'telephone', 'id': 124},\n",
       " {'name': 'trolley', 'id': 125},\n",
       " {'name': 'head phone', 'id': 126},\n",
       " {'name': 'sports car', 'id': 127},\n",
       " {'name': 'stop sign', 'id': 128},\n",
       " {'name': 'dessert', 'id': 129},\n",
       " {'name': 'scooter', 'id': 130},\n",
       " {'name': 'stroller', 'id': 131},\n",
       " {'name': 'crane', 'id': 132},\n",
       " {'name': 'remote', 'id': 133},\n",
       " {'name': 'refrigerator', 'id': 134},\n",
       " {'name': 'oven', 'id': 135},\n",
       " {'name': 'lemon', 'id': 136},\n",
       " {'name': 'duck', 'id': 137},\n",
       " {'name': 'baseball bat', 'id': 138},\n",
       " {'name': 'surveillance camera', 'id': 139},\n",
       " {'name': 'cat', 'id': 140},\n",
       " {'name': 'jug', 'id': 141},\n",
       " {'name': 'broccoli', 'id': 142},\n",
       " {'name': 'piano', 'id': 143},\n",
       " {'name': 'pizza', 'id': 144},\n",
       " {'name': 'elephant', 'id': 145},\n",
       " {'name': 'skateboard', 'id': 146},\n",
       " {'name': 'surfboard', 'id': 147},\n",
       " {'name': 'gun', 'id': 148},\n",
       " {'name': 'skating and skiing shoes', 'id': 149},\n",
       " {'name': 'gas stove', 'id': 150},\n",
       " {'name': 'donut', 'id': 151},\n",
       " {'name': 'bow tie', 'id': 152},\n",
       " {'name': 'carrot', 'id': 153},\n",
       " {'name': 'toilet', 'id': 154},\n",
       " {'name': 'kite', 'id': 155},\n",
       " {'name': 'strawberry', 'id': 156},\n",
       " {'name': 'other balls', 'id': 157},\n",
       " {'name': 'shovel', 'id': 158},\n",
       " {'name': 'pepper', 'id': 159},\n",
       " {'name': 'computer box', 'id': 160},\n",
       " {'name': 'toilet paper', 'id': 161},\n",
       " {'name': 'cleaning products', 'id': 162},\n",
       " {'name': 'chopsticks', 'id': 163},\n",
       " {'name': 'microwave', 'id': 164},\n",
       " {'name': 'pigeon', 'id': 165},\n",
       " {'name': 'baseball', 'id': 166},\n",
       " {'name': 'cutting board', 'id': 167},\n",
       " {'name': 'coffee table', 'id': 168},\n",
       " {'name': 'side table', 'id': 169},\n",
       " {'name': 'scissors', 'id': 170},\n",
       " {'name': 'marker', 'id': 171},\n",
       " {'name': 'pie', 'id': 172},\n",
       " {'name': 'ladder', 'id': 173},\n",
       " {'name': 'snowboard', 'id': 174},\n",
       " {'name': 'cookies', 'id': 175},\n",
       " {'name': 'radiator', 'id': 176},\n",
       " {'name': 'fire hydrant', 'id': 177},\n",
       " {'name': 'basketball', 'id': 178},\n",
       " {'name': 'zebra', 'id': 179},\n",
       " {'name': 'grape', 'id': 180},\n",
       " {'name': 'giraffe', 'id': 181},\n",
       " {'name': 'potato', 'id': 182},\n",
       " {'name': 'sausage', 'id': 183},\n",
       " {'name': 'tricycle', 'id': 184},\n",
       " {'name': 'violin', 'id': 185},\n",
       " {'name': 'egg', 'id': 186},\n",
       " {'name': 'fire extinguisher', 'id': 187},\n",
       " {'name': 'candy', 'id': 188},\n",
       " {'name': 'fire truck', 'id': 189},\n",
       " {'name': 'billards', 'id': 190},\n",
       " {'name': 'converter', 'id': 191},\n",
       " {'name': 'bathtub', 'id': 192},\n",
       " {'name': 'wheelchair', 'id': 193},\n",
       " {'name': 'golf club', 'id': 194},\n",
       " {'name': 'briefcase', 'id': 195},\n",
       " {'name': 'cucumber', 'id': 196},\n",
       " {'name': 'cigar / cigarette ', 'id': 197},\n",
       " {'name': 'paint brush', 'id': 198},\n",
       " {'name': 'pear', 'id': 199},\n",
       " {'name': 'heavy truck', 'id': 200},\n",
       " {'name': 'hamburger', 'id': 201},\n",
       " {'name': 'extractor', 'id': 202},\n",
       " {'name': 'extension cord', 'id': 203},\n",
       " {'name': 'tong', 'id': 204},\n",
       " {'name': 'tennis racket', 'id': 205},\n",
       " {'name': 'folder', 'id': 206},\n",
       " {'name': 'american football', 'id': 207},\n",
       " {'name': 'earphone', 'id': 208},\n",
       " {'name': 'mask', 'id': 209},\n",
       " {'name': 'kettle', 'id': 210},\n",
       " {'name': 'tennis', 'id': 211},\n",
       " {'name': 'ship', 'id': 212},\n",
       " {'name': 'swing', 'id': 213},\n",
       " {'name': 'coffee machine', 'id': 214},\n",
       " {'name': 'slide', 'id': 215},\n",
       " {'name': 'carriage', 'id': 216},\n",
       " {'name': 'onion', 'id': 217},\n",
       " {'name': 'green beans', 'id': 218},\n",
       " {'name': 'projector', 'id': 219},\n",
       " {'name': 'frisbee', 'id': 220},\n",
       " {'name': 'washing machine / drying machine', 'id': 221},\n",
       " {'name': 'chicken', 'id': 222},\n",
       " {'name': 'printer', 'id': 223},\n",
       " {'name': 'watermelon', 'id': 224},\n",
       " {'name': 'saxophone', 'id': 225},\n",
       " {'name': 'tissue', 'id': 226},\n",
       " {'name': 'toothbrush', 'id': 227},\n",
       " {'name': 'ice cream', 'id': 228},\n",
       " {'name': 'hot air ballon', 'id': 229},\n",
       " {'name': 'cello', 'id': 230},\n",
       " {'name': 'french fries', 'id': 231},\n",
       " {'name': 'scale', 'id': 232},\n",
       " {'name': 'trophy', 'id': 233},\n",
       " {'name': 'cabbage', 'id': 234},\n",
       " {'name': 'hot dog', 'id': 235},\n",
       " {'name': 'blender', 'id': 236},\n",
       " {'name': 'peach', 'id': 237},\n",
       " {'name': 'rice', 'id': 238},\n",
       " {'name': 'wallet / purse', 'id': 239},\n",
       " {'name': 'volleyball', 'id': 240},\n",
       " {'name': 'deer', 'id': 241},\n",
       " {'name': 'goose', 'id': 242},\n",
       " {'name': 'tape', 'id': 243},\n",
       " {'name': 'tablet', 'id': 244},\n",
       " {'name': 'cosmetics', 'id': 245},\n",
       " {'name': 'trumpet', 'id': 246},\n",
       " {'name': 'pineapple', 'id': 247},\n",
       " {'name': 'golf ball', 'id': 248},\n",
       " {'name': 'ambulance', 'id': 249},\n",
       " {'name': 'parking meter', 'id': 250},\n",
       " {'name': 'mango', 'id': 251},\n",
       " {'name': 'key', 'id': 252},\n",
       " {'name': 'hurdle', 'id': 253},\n",
       " {'name': 'fishing rod', 'id': 254},\n",
       " {'name': 'medal', 'id': 255},\n",
       " {'name': 'flute', 'id': 256},\n",
       " {'name': 'brush', 'id': 257},\n",
       " {'name': 'penguin', 'id': 258},\n",
       " {'name': 'megaphone', 'id': 259},\n",
       " {'name': 'corn', 'id': 260},\n",
       " {'name': 'lettuce', 'id': 261},\n",
       " {'name': 'garlic', 'id': 262},\n",
       " {'name': 'swan', 'id': 263},\n",
       " {'name': 'helicopter', 'id': 264},\n",
       " {'name': 'green onion', 'id': 265},\n",
       " {'name': 'sandwich', 'id': 266},\n",
       " {'name': 'nuts', 'id': 267},\n",
       " {'name': 'speed limit sign', 'id': 268},\n",
       " {'name': 'induction cooker', 'id': 269},\n",
       " {'name': 'broom', 'id': 270},\n",
       " {'name': 'trombone', 'id': 271},\n",
       " {'name': 'plum', 'id': 272},\n",
       " {'name': 'rickshaw', 'id': 273},\n",
       " {'name': 'goldfish', 'id': 274},\n",
       " {'name': 'kiwi fruit', 'id': 275},\n",
       " {'name': 'router / modem', 'id': 276},\n",
       " {'name': 'poker card', 'id': 277},\n",
       " {'name': 'toaster', 'id': 278},\n",
       " {'name': 'shrimp', 'id': 279},\n",
       " {'name': 'sushi', 'id': 280},\n",
       " {'name': 'cheese', 'id': 281},\n",
       " {'name': 'notepaper', 'id': 282},\n",
       " {'name': 'cherry', 'id': 283},\n",
       " {'name': 'pliers', 'id': 284},\n",
       " {'name': 'cd', 'id': 285},\n",
       " {'name': 'pasta', 'id': 286},\n",
       " {'name': 'hammer', 'id': 287},\n",
       " {'name': 'cue', 'id': 288},\n",
       " {'name': 'avocado', 'id': 289},\n",
       " {'name': 'hami melon', 'id': 290},\n",
       " {'name': 'flask', 'id': 291},\n",
       " {'name': 'mushroom', 'id': 292},\n",
       " {'name': 'screwdriver', 'id': 293},\n",
       " {'name': 'soap', 'id': 294},\n",
       " {'name': 'recorder', 'id': 295},\n",
       " {'name': 'bear', 'id': 296},\n",
       " {'name': 'eggplant', 'id': 297},\n",
       " {'name': 'board eraser', 'id': 298},\n",
       " {'name': 'coconut', 'id': 299},\n",
       " {'name': 'tape measure / ruler', 'id': 300},\n",
       " {'name': 'pig', 'id': 301},\n",
       " {'name': 'showerhead', 'id': 302},\n",
       " {'name': 'globe', 'id': 303},\n",
       " {'name': 'chips', 'id': 304},\n",
       " {'name': 'steak', 'id': 305},\n",
       " {'name': 'crosswalk sign', 'id': 306},\n",
       " {'name': 'stapler', 'id': 307},\n",
       " {'name': 'camel', 'id': 308},\n",
       " {'name': 'formula 1 car', 'id': 309},\n",
       " {'name': 'pomegranate', 'id': 310},\n",
       " {'name': 'dishwasher', 'id': 311},\n",
       " {'name': 'crab', 'id': 312},\n",
       " {'name': 'hoverboard', 'id': 313},\n",
       " {'name': 'meatball', 'id': 314},\n",
       " {'name': 'rice cooker', 'id': 315},\n",
       " {'name': 'tuba', 'id': 316},\n",
       " {'name': 'calculator', 'id': 317},\n",
       " {'name': 'papaya', 'id': 318},\n",
       " {'name': 'antelope', 'id': 319},\n",
       " {'name': 'parrot', 'id': 320},\n",
       " {'name': 'seal', 'id': 321},\n",
       " {'name': 'butterfly', 'id': 322},\n",
       " {'name': 'dumbbell', 'id': 323},\n",
       " {'name': 'donkey', 'id': 324},\n",
       " {'name': 'lion', 'id': 325},\n",
       " {'name': 'urinal', 'id': 326},\n",
       " {'name': 'dolphin', 'id': 327},\n",
       " {'name': 'electric drill', 'id': 328},\n",
       " {'name': 'hairdrier', 'id': 329},\n",
       " {'name': 'egg tart', 'id': 330},\n",
       " {'name': 'jellyfish', 'id': 331},\n",
       " {'name': 'treadmill', 'id': 332},\n",
       " {'name': 'lighter', 'id': 333},\n",
       " {'name': 'grapefruit', 'id': 334},\n",
       " {'name': 'game board', 'id': 335},\n",
       " {'name': 'mop', 'id': 336},\n",
       " {'name': 'radish', 'id': 337},\n",
       " {'name': 'baozi', 'id': 338},\n",
       " {'name': 'target', 'id': 339},\n",
       " {'name': 'french', 'id': 340},\n",
       " {'name': 'spring rolls', 'id': 341},\n",
       " {'name': 'monkey', 'id': 342},\n",
       " {'name': 'rabbit', 'id': 343},\n",
       " {'name': 'pencil case', 'id': 344},\n",
       " {'name': 'yak', 'id': 345},\n",
       " {'name': 'red cabbage', 'id': 346},\n",
       " {'name': 'binoculars', 'id': 347},\n",
       " {'name': 'asparagus', 'id': 348},\n",
       " {'name': 'barbell', 'id': 349},\n",
       " {'name': 'scallop', 'id': 350},\n",
       " {'name': 'noddles', 'id': 351},\n",
       " {'name': 'comb', 'id': 352},\n",
       " {'name': 'dumpling', 'id': 353},\n",
       " {'name': 'oyster', 'id': 354},\n",
       " {'name': 'table tennis paddle', 'id': 355},\n",
       " {'name': 'cosmetics brush / eyeliner pencil', 'id': 356},\n",
       " {'name': 'chainsaw', 'id': 357},\n",
       " {'name': 'eraser', 'id': 358},\n",
       " {'name': 'lobster', 'id': 359},\n",
       " {'name': 'durian', 'id': 360},\n",
       " {'name': 'okra', 'id': 361},\n",
       " {'name': 'lipstick', 'id': 362},\n",
       " {'name': 'cosmetics mirror', 'id': 363},\n",
       " {'name': 'curling', 'id': 364},\n",
       " {'name': 'table tennis', 'id': 365}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b2a83ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'height': 512,\n",
       " 'id': 420917,\n",
       " 'license': 5,\n",
       " 'width': 769,\n",
       " 'file_name': 'patch8/objects365_v1_00420917.jpg',\n",
       " 'url': ''}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "def save_cropped_images(annotations_path, images_dir, output_dir, target_labels):\n",
    "    \"\"\"\n",
    "    Extracts and saves cropped images based on given labels from COCO-format Object365 data.\n",
    "\n",
    "    Args:\n",
    "        annotations_path (str): Path to the COCO JSON annotation file.\n",
    "        images_dir (str): Directory containing original images.\n",
    "        output_dir (str): Directory where cropped images will be saved.\n",
    "        target_labels (list): List of labels to extract and save.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load COCO JSON annotations\n",
    "    with open(annotations_path, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Create a mapping from category ID to category name\n",
    "    category_mapping = {cat[\"id\"]: cat[\"name\"] for cat in coco_data[\"categories\"]}\n",
    "\n",
    "    # Reverse mapping (label -> category_id)\n",
    "    label_to_category_id = {v: k for k, v in category_mapping.items()}\n",
    "\n",
    "    # Filter annotations based on target labels\n",
    "    target_category_ids = {label_to_category_id[label] for label in target_labels if label in label_to_category_id}\n",
    "\n",
    "    # Create a mapping from image ID to file name\n",
    "    image_id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in coco_data[\"images\"]}\n",
    "\n",
    "    # Create output directories for each label\n",
    "    for label in target_labels:\n",
    "        os.makedirs(os.path.join(output_dir, label), exist_ok=True)\n",
    "\n",
    "    # Iterate over annotations and crop images\n",
    "    for ann in coco_data[\"annotations\"]:\n",
    "        category_id = ann[\"category_id\"]\n",
    "        if category_id in target_category_ids:\n",
    "            image_id = ann[\"image_id\"]\n",
    "            bbox = ann[\"bbox\"]  # Format: [x, y, width, height]\n",
    "\n",
    "            # Load the corresponding image\n",
    "            image_filename = image_id_to_filename.get(image_id)\n",
    "            if not image_filename:\n",
    "                continue\n",
    "\n",
    "            image_path = os.path.join(images_dir, image_filename)\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Warning: Image {image_path} not found.\")\n",
    "                continue\n",
    "\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Warning: Failed to load image {image_path}.\")\n",
    "                continue\n",
    "\n",
    "            # Crop the object\n",
    "            x, y, w, h = map(int, bbox)\n",
    "            cropped_image = image[y:y+h, x:x+w]\n",
    "\n",
    "            # Skip empty crops\n",
    "            if cropped_image.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Save the cropped image in the corresponding label folder\n",
    "            label_name = category_mapping[category_id]\n",
    "            save_path = os.path.join(output_dir, label_name, f\"{image_id}_{ann['id']}.jpg\")\n",
    "            cv2.imwrite(save_path, cropped_image)\n",
    "\n",
    "            print(f\"Saved: {save_path}\")\n",
    "\n",
    "    print(\"Cropping and saving completed!\")\n",
    "\n",
    "annotations_path = '/net/acadia12a/data/samuel/objects365v1/zsy_objv1_train_xdino.json'\n",
    "images_dir = '/net/acadia12a/data/samuel/objects365v1/train'\n",
    "output_dir = '/net/acadia14a/data/sparsh/Relabeling/dinov2_data'\n",
    "target_labels = ['boat', 'suv', 'car', 'van', 'bus', 'motorcycle', 'truck', 'train', 'bicycle']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip3",
   "language": "python",
   "name": "clip3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
